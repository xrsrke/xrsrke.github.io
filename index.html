<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Phuc Nguyen</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <main class="container">
        <header>
            <img src="avatar.jpg" alt="Phuc Nguyen" class="avatar">
            <h1 class="name">Phuc Nguyen</h1>
            <p class="subtitle">High school dropout. Research Engineer at Hugging Face.</p>
            <p class="title">DMs open. The best reach me is <a href="https://x.com/xariusrke">twitter</a>, discord: <a href="https://discordapp.com/users/930102195330900009">neuralink</a>, or email: phucnh791 [at] gmail [dot] com</p>
        </header>

        <section class="about">
            <p>I'm currently a research engineer at Hugging Face in ü•ê Paris, where I'm part of the distributed training team <a href="https://github.com/huggingface/nanotron">nanotron</a> and work on <span class="tooltip">various research reproduction efforts<span class="tooltip-text">These Twitter threads document my research experiments - scroll through earlier and later tweets to see the full journey and discoveries</span></span> on the Hugging Face science team (<a href="https://x.com/xariusrke/status/1826669126955278401">FP8 research</a>, <a href="https://x.com/xariusrke/status/1805963548109238557">Infini-Attention</a>, <a href="https://x.com/xariusrke/status/1935999178288545832">MoE's Expert Parallelism</a>, <a href="https://x.com/xariusrke/status/1894461976606093567">DoMiNo</a>, <a href="https://x.com/xariusrke/status/1757415237089677677">DoReMi</a>).</p>
            <p>I maintain a <a href="https://x.com/xariusrke/status/1935978626698371096">life-long learning progress thread</a> where I've shared my study notes over the past two years (<a href="https://x.com/xariusrke/status/1826669126955278401">3D parallelism</a>).</p>
            <p>Before Hugging Face, I designed <a href="https://x.com/xariusrke/status/1613295335295823873">a study plan</a> that spanned across many subjects and then consistently studied from 3:30 AM to 3:30 PM, then went to sleep from 5:20 PM to 3:00 AM and repeated for 2 years.</p>
        </section>

        <section class="projects">
            <h2>Selected Work</h2>
            <article class="project">
                <h3><a href="https://x.com/xariusrke/status/1826669126955278401">Stable FP8 pretraining recipe for Transformer</a></h3>
                <p>Found two stable FP8 pretraining recipes that pretrained a LLaMA 2 architecture in FP8 for both forward and backward passes, as well as both momentums (50% memory reduction), while matching the standard BF16 mixed-precision baseline after <a href="https://x.com/xariusrke/status/1838592991352504761">100B tokens</a>.<br>
                    <small style="color: #666; margin-left: 2em;">Recipe 1: <a href="https://x.com/xariusrke/status/1826669126955278401" style="color: #888;">with architecture and optimizer modification</a></small><br>
                    <small style="color: #666; margin-left: 2em;">Recipe 2: <a href="https://x.com/xariusrke/status/1862237702118150363" style="color: #888;">ablated of recipe 1 without architecture modification</a></small></p>
            </article>
            <article class="project">
                <h3><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high-level_overview">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></h3>
                <p>Nouamane Tazi, Ferdinand Mom, Haojun Zhao, <u>Phuc Nguyen</u>, Mohamed Mekkouri, Leandro Werra, Thomas Wolf.</p>
            </article>
            <article class="project">
                <h3><a href="https://github.com/huggingface/nanotron">nanotron - Minimalistic LLM 3D-parallelism training</a> ‚≠ê 2.1k</h3>
                <p>A 3D parallelism distributed training library used for smollm and fineweb.</p>
            </article>
            <article class="project">
                <h3><a href="https://github.com/xrsrke/pipegoose">pipegoose - 3D parallelism</a></h3>
                <p>Implemented a 3D parallelism library from scratch with ZeRO-1.</p>
            </article>

            <article class="project">
                <h3><a href="https://huggingface.co/blog/infini-attention">A failed experiment: Infini-Attention, and why we should keep trying?</a></h3>
                <p>Research reproduction of the Infini-Attention paper. TLDR: Infini-Attention's performance gets worse as we increase the number of times we compress the memory. To the best of our knowledge, ring attention, YaRN, and rope scaling are still the best ways for extending a pretrained model to a longer context length.</p>
            </article>
        </section>

        <section class="projects">
            <h2>Selected Study Notes</h2>
            <article class="project">
                <h3><a href="https://projectfoundation.notion.site/Megatron-LM-OSLO-DeepSped-and-FSDP-s-implementation-details-dbfb2bacc65d42cda0a0633ab1bdc994">A complete breakdown of the implementation details of Megatron-LM, torchgpipe, and OSLO codebases</a></h3>
                <p>3D parallelism, Sequence Parallelism, ZeRO-1, Mixture of Experts, FSDP, and other engineering optimizations in popular codebases.</p>
            </article>
            <article class="project">
                <h3><a href="https://phrygian-alarm-029.notion.site/Clean-FP8-LM-Training-FP8-Large-Language-Models-c1e88b30bbe64ed9b6d5df124aae267e?source=copy_link">FP8-LM: Training FP8 Large Language Models</a></h3>
            </article>
            <article class="project">
                <h3><a href="https://projectfoundation.notion.site/Horovod-Elastic-Training-and-Fault-Tolerance-ee1d5738f4d64961b689662229b5c76d?source=copy_link">Horovod - Elastic Training and Fault-Tolerance</a></h3>
            </article>
            <article class="project">
                <h3><a href="https://phrygian-alarm-029.notion.site/notes-reading-7b6946b8f4074771b9922654d61075d6?source=copy_link">All other unorganized notes</a></h3>
            </article>
        </section>

        <section class="talks">
            <h2>Talks</h2>
            <article class="project">
                <h3><a href="https://x.com/xariusrke/status/1862237702118150363">Gave an oral presentation at France's Jean Zay supercomputer facility, part of the France's National Scientific Research Center (CNRS)</a></h3>
            </article>
        </section>

        <section class="contact">
            <h2>Get in Touch</h2>
            <p>
                <a href="https://x.com/xariusrke">Twitter</a> ‚Ä¢ 
                <a href="https://github.com/xrsrke">GitHub</a> ‚Ä¢ 
                <a href="https://discordapp.com/users/930102195330900009">Discord</a> ‚Ä¢ 
                <a href="https://www.twitch.tv/xrsrke">Twitch</a> ‚Ä¢ 
                <a href="mailto:phucnh791@gmail.com">Email</a> ‚Ä¢ 
                <a href="https.linkedin.com/in/phuc-nguyen-418a4219a/">LinkedIn</a> ‚Ä¢ 
                <a href="https://docs.google.com/document/d/1pIbeiAnqsSY9sEtQRHjGx_1t2W_AH8Cs4OcUerA2rPY/edit?tab=t.0">CV</a>
            </p>
        </section>
    </main>
</body>
</html>